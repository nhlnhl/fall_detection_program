{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = np.genfromtxt('../dataset/training_data_after_ensemble.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            title = 'Confusion Matrix, without Normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print('Normalized Confusion Matrix')\n",
    "    else:\n",
    "        print('Confusion Matrix, without Normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\", fontsize=15,\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_original_input = 11     # 입력데이터의 속성 수\n",
    "n_input = 11\n",
    "n_step = 1       # 한 번에 입력할 데이터 수\n",
    "n_class = 11      # 결과값의 종류 수\n",
    "batch_size = 1   # 배치 크기 (ex. n_step개 데이터 batch_size묶음이 1배치)\n",
    "n_epoch = 30     # 총 학습 횟수\n",
    "\n",
    "n_hidden = 128\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch 당 batch 수 계산\n",
    "train_batch = train.shape[0]//(batch_size*n_step)\n",
    "print(train_batch)\n",
    "test_batch = test.shape[0]//(batch_size*n_step)\n",
    "print(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "x_train = train[:, :n_original_input]\n",
    "y_train_temp = train[:, -1]\n",
    "\n",
    "#x_train = x_train[:, [0, 1, 2, 3, 4, 5, 6, 7, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 59, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 102, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 132, 133, 135, 136, 137]]\n",
    "\n",
    "#sess = tf.Session()\n",
    "y_train_temp = np.subtract(y_train_temp, np.ones(y_train_temp.shape))\n",
    "#y_train_temp = tf.one_hot(y_train_temp, depth=n_class).eval(session=sess)\n",
    "y_train_temp = np.array(y_train_temp).reshape(-1, 1)\n",
    "enc.fit(y_train_temp)\n",
    "y_train_temp = enc.transform(y_train_temp).toarray()\n",
    "\n",
    "y_train = np.zeros([train_batch, n_class])\n",
    "for i in range(train_batch):\n",
    "    y_train[i] = y_train_temp[n_step * i]\n",
    "\n",
    "x_train = x_train.tolist()\n",
    "y_train = y_train.tolist()\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test[:, :n_original_input]\n",
    "y_test_temp = test[:, -1]\n",
    "\n",
    "#x_test = x_test[:, [0, 1, 2, 3, 4, 5, 6, 7, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 59, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 102, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 132, 133, 135, 136, 137]]\n",
    "\n",
    "#sess = tf.Session()\n",
    "y_test_temp = np.subtract(y_test_temp, np.ones(y_test_temp.shape))\n",
    "#y_test_temp = tf.one_hot(y_test_temp, depth=n_class).eval(session=sess)\n",
    "y_test_temp = np.array(y_test_temp).reshape(-1, 1)\n",
    "enc.fit(y_test_temp)\n",
    "y_test_temp = enc.transform(y_test_temp).toarray()\n",
    "\n",
    "y_test = np.zeros([test_batch, n_class])\n",
    "for i in range(test_batch):\n",
    "    y_test[i] = y_test_temp[n_step * i]\n",
    "\n",
    "x_test = x_test.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "print(len(x_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 iteration의 batch를 반환하는 함수\n",
    "def get_batch(x, y, iteration, size = batch_size):\n",
    "  start = size*iteration\n",
    "  batch_x = []\n",
    "  batch_y = []\n",
    "  for i in range(size):\n",
    "    batch_x.append(x[(start+i)*n_step:(start+i+1)*n_step])\n",
    "    batch_y.append(y[start+i])\n",
    "  return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 구성\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input], name='x')\n",
    "Y = tf.placeholder(tf.int32, [None, n_class], name='y')\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([n_hidden, n_class]))\n",
    "b = tf.Variable(tf.truncated_normal([n_class]))\n",
    "\n",
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.8)\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "cell2 = tf.nn.rnn_cell.DropoutWrapper(cell2, output_keep_prob=0.8)\n",
    "cell3 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "cell3 = tf.nn.rnn_cell.DropoutWrapper(cell3, output_keep_prob=0.8)\n",
    "cell4 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "cell4 = tf.nn.rnn_cell.DropoutWrapper(cell4, output_keep_prob=0.8)\n",
    "cell5 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "cell5 = tf.nn.rnn_cell.DropoutWrapper(cell5, output_keep_prob=0.8)\n",
    "cell6 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2, cell3, cell4, cell5, cell6])\n",
    "\n",
    "output, state = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\n",
    "output = tf.transpose(output, [1,0,2])\n",
    "output = output[-1]\n",
    "\n",
    "model = tf.matmul(output, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 세션 실행\n",
    "with tf.Session() as sess:\n",
    "  tf.global_variables_initializer().run()\n",
    "\n",
    "  # 학습\n",
    "  for i in range(n_epoch):\n",
    "    total_loss = 0\n",
    "    for j in range(train_batch):\n",
    "      batch_x, batch_y = get_batch(x_train, y_train, j)\n",
    "      _, loss = sess.run([optimizer, cost], feed_dict={X: batch_x, Y: batch_y})\n",
    "      total_loss += loss\n",
    "    print('epoch: %d, avg.loss: %f'%(i+1, total_loss/train_batch)) \n",
    "\n",
    "  # 모델 테스트\n",
    "  total_acc = 0\n",
    "  for k in range(train_batch):\n",
    "    test_x, test_y = get_batch(x_train, y_train, k)\n",
    "    acc = sess.run(accuracy, feed_dict={X: test_x, Y: test_y})\n",
    "    total_acc += acc\n",
    "  print('---train accuracy: %f,'%(total_acc/train_batch))\n",
    "\n",
    "  # 테스트\n",
    "  total_acc = 0\n",
    "  for k in range(test_batch):\n",
    "    test_x, test_y = get_batch(x_test, y_test, k)\n",
    "    acc = sess.run(accuracy, feed_dict={X: test_x, Y: test_y})\n",
    "    total_acc += acc\n",
    "  print('---test accuracy: %f,'%(total_acc/test_batch))\n",
    "    \n",
    "  # confusion matrix\n",
    "  '''\n",
    "  train_x, train_y = get_batch(x_train, y_train, 0, train_batch)\n",
    "  cm = tf.confusion_matrix(tf.argmax(train_y, 1), tf.argmax(sess.run(model, feed_dict={X: train_x, Y: train_y}), 1), n_class)\n",
    "  test_x, test_y = get_batch(x_test, y_test, 0, test_batch)\n",
    "  cm += tf.confusion_matrix(tf.argmax(test_y, 1), tf.argmax(sess.run(model, feed_dict={X: test_x, Y: test_y}), 1), n_class)\n",
    "  # df_cm = pd.DataFrame(sess.run(cm), range(n_class), range(n_class))\n",
    "  df_cm = pd.DataFrame(sess.run(cm), index=[\"Falling forward using hands\", \"Falling forward using knees\", \"Falling backwards\", \"Falling sideward\", \"Falling sitting in empty chair\", \"Walking\", \"Standing\", \"Sitting\", \"Picking up an object\", \"Jumping\", \"Laying\"], columns=[\"Falling forward using hands\", \"Falling forward using knees\", \"Falling backwards\", \"Falling sideward\", \"Falling sitting in empty chair\", \"Walking\", \"Standing\", \"Sitting\", \"Picking up an object\", \"Jumping\", \"Laying\"])\n",
    "  # plt.figure(figsize = (10,7))\n",
    "  sn.set(font_scale=1.4) # for label size\n",
    "  sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}) # font size\n",
    "  '''\n",
    "  np.set_printoptions(precision=2)\n",
    "  test_x, test_y = get_batch(x_test, y_test, 0, test_batch)\n",
    "  y_pred = sess.run(model, feed_dict={X: test_x, Y: test_y})\n",
    "  y_pred = np.argmax(y_pred, axis=1).reshape(-1)\n",
    "  print(y_pred.shape)\n",
    "  print(y_pred)\n",
    "  classes = ['Falling forward using hands', 'Falling forward using knees', 'Falling backwards', 'Falling sideward', 'Falling sitting in empty chair', 'Walking', 'Standing', 'Sitting', 'Picking up an object', 'Jumping', 'Laying']\n",
    "\n",
    "  y_act = np.argmax(y_test, axis=1).reshape(-1)\n",
    "  print(y_act.shape)\n",
    "  print(y_act)\n",
    "\n",
    "  # Plot non-normalized confusion matrix\n",
    "  plot_confusion_matrix(y_act, y_pred, classes=classes, title='Confusion Matrix, without Normalization')\n",
    "\n",
    "  # Plot normalized confusion matrix\n",
    "  plot_confusion_matrix(y_act, y_pred, classes=classes, normalize=True, title='Normalized Confusion Matrix')\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "  sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
